#list project paths separated by commas
ProjectRepository=/home/only/Workspaces/Bachelor/ideprojects/plugin/HAnS
#C:/studies/defectprediction/blender,C:/studies/defectprediction/busybox,C:/studies/defectprediction/emacs,C:/studies/defectprediction/gimp,C:/studies/defectprediction/gnumeric,C:/studies/defectprediction/gnuplot,C:/studies/defectprediction/irssi,C:/studies/defectprediction/libxml2,C:/studies/defectprediction/lighttpd1.4,C:/studies/defectprediction/MPSolve,C:/studies/defectprediction/parrot,C:/studies/defectprediction/vim,C:/studies/defectprediction/marlin
# C:/studies/defectprediction/marlin
#,C:/studies/defectprediction/busybox,C:/studies/defectprediction/emacs,C:/studies/defectprediction/gimp,C:/studies/defectprediction/gnumeric,C:/studies/defectprediction/gnuplot,C:/studies/defectprediction/irssi,C:/studies/defectprediction/libxml2,C:/studies/defectprediction/lighttpd1.4,C:/studies/defectprediction/MPSolve,C:/studies/defectprediction/parrot
#C:/studies/ClaferConfigurator,C:/studies/ClaferIDE,C:/studies/ClaferMooVisualizer,C:/studies/ClaferToolsUICommonPlatform,C:/studies/Marlin,C:/studies/defectprediction/blender,C:/studies/defectprediction/busybox,C:/studies/defectprediction/emacs,C:/studies/defectprediction/gimp,C:/studies/defectprediction/gnumeric,C:/studies/defectprediction/gnuplot,C:/studies/defectprediction/irssi,C:/studies/defectprediction/libxml2,C:/studies/defectprediction/lighttpd1.4,C:/studies/defectprediction/MPSolve,C:/studies/defectprediction/parrot
#C:/studies/ClaferConfigurator,C:/studies/ClaferIDE,C:/studies/ClaferMooVisualizer,C:/studies/ClaferToolsUICommonPlatform,C:/studies/Marlin
#C:/studies/defectprediction/blender
#C:/studies/defectprediction/busybox
#C:/studies/defectprediction/emacs
#C:/studies/defectprediction/gimp
#C:/studies/defectprediction/gnumeric
#C:/studies/defectprediction/gnuplot
#C:/studies/defectprediction/irssi
#C:/studies/defectprediction/libxml2
#C:/studies/defectprediction/lighttpd1.4
#C:/studies/defectprediction/MPSolve
#C:/studies/defectprediction/parrot
#C:/studies/defectprediction/vim
#for each project above, give it a short alias
ProjectShortNames=hans
 #blender,busybox,emacs,gimp,gnumeric,gnuplot,irssi,libxml2,lighttpd,mpsolve,parrot,vim,marlin
#,busybox,emacs,gimp,gnumeric,gnuplot,irssi,libxml2,lighttpd,mpsolve,parrot
#config,ide,viz,tools,marlin,blender,busybox,emacs,gimp,gnumeric,gnuplot,irssi,libxml2,lighttpd,mpsolve,parrot
#These names must match the folder names of the projects because they are used somtimes to automatically read CSV files of results
ProjectNamesList=hans
#blender,busybox,emacs,gimp,gnumeric,gnuplot,irssi,libxml2,lighttpd1.4,MPSolve,parrot,vim,marlin
#ClaferConfigurator,ClaferIDE,ClaferMooVisualizer,ClaferToolsUICommonPlatform,Marlin,blender,busybox,emacs,gimp,gnumeric,gnuplot,irssi,libxml2,lighttpd1.4,MPSolve,parrot
#Short names will be used in charts for analyszing results
ProjectShortNameList=hans
#blender,busybox,emacs,gimp,gnumeric,gnuplot,irssi,libxml2,lighttpd,mpsolve,parrot,vim,marlin
#config,ide,viz,tools,marlin,blender,busybox,emacs,gimp,gnumeric,gnuplot,irssi,libxml2,lighttpd,mpsolve,parrot
#ProjectTypes refers to whether the commits to consider are only for the simulated development or all commits. enum values: SIMULATION, REGULAR
#THe SIMULATION value is no loger necesssary since even for the claferwebtools projects we consider all projects instead of the simulation ones only
ProjectType=REGULAR
#This is where results will be stored and copies of the projects kept for analysis
AnalysisDirectory=/home/only/Workspaces/Bachelor/ideprojects/featraceranalysis
#Specify what annotations are used in all the projects above, either embedded annotations (EA) or preprocessors (CPP). ENUM values: EA, CPP
ProjectAnnotationType=EA
#Specify whether to generate ARFF datasets for trainig and testing
GenerateARRFFiles=false
CopyRepositoriesToTargetAnalysisFolder=false
CommitsToExecute=0
StartingCommitIndex=1
ReadAllAssetsFromEachCommit=false
PrintAssetMappings=true
ExperimentStartingCommit=0
TrainingDataIncludesUnMappedAssets=true
AssetTypesToPredict=FOLDER,FILE,FRAGMENT
#What should FeatRacer do.
#D-generate Data
#GM-generate metrics data for database based data;
#GDT-Generate ARFF datasets from the metroc data generated
#EDB-run classifiers on the generated ARFF datasets
#EDBTopN-run experiment for TopN features

#CM-Generate Commit Metrics data e.g., lines added, removed, per commit
#CMU-Commit Metrics Update--update commit metrics to include asset-based metrics such as ratio of annotated assets, folders, files, fragments, and lines
#RU-Results Updater-inserts prediction results from CSV files into DB
#GMCM-Generate Mappings for Commit Metrics--generates CSVs shwoing commit metrics mapped to prediction results
#CROF-Combine Results into One File--read all combined CSVs into one file
#RG-Report Generator
#LTG-Latex Table Generator--generates a sumary of results for copying into latex
#FC-Feature Cleaner--clean features by removing unwanted charaters such as &&, ||, *, whitespaces, etc. This is important to ensure that WEKA ARFF files work.
#LDG-Lucene Data Generator--Generate Lucene data
#CLD-Combine Luecene Data for all projects
#DSSG-DataSet Stats Generator--generate stats about number of instances, labels, cardinlaity, etc
#FS-Feature Selector--rank the ML features e.g, COMM, DDEV, CSV, DFMA, etc according to their power to predict feature traces
#CFS-Combine Feature Selection summary files for all projects
#CNTop-Combine N top feature files
#AWC-Read Annotations from the given git project without running through the revision history (all commits)
#DP-defect prediction
#CMDP-Calculate metrics for Defect prediction
#GPCD-Group projects for clone detection
ExecutionMethod=EDB
#Generate Lucene data
GenerateLuceneData=false
#Similarity threhold for Lucene values
LuceneThreshold=0.7
#Selecting this option will Calculate metrics e.g., tangling degree, etc
CalculateMetrics=true
#Specify if commit for metrics are from a CSv file or obtained from a project's revision history
UseMetricCommitsFromCSV=true
#List of CSv files with specific commits to generate metrics for. CSV files must be in the same order as the projects above
#These files are used to get commit list for release based prediction
CommitCSVFiles=C:/studies/defectprediction/structure_metrics_files/blender.csv,C:/studies/defectprediction/structure_metrics_files/busybox.csv,C:/studies/defectprediction/structure_metrics_files/emacs.csv,C:/studies/defectprediction/structure_metrics_files/gimp.csv,C:/studies/defectprediction/structure_metrics_files/gnumeric.csv,C:/studies/defectprediction/structure_metrics_files/gnuplot.csv,C:/studies/defectprediction/structure_metrics_files/irssi.csv,C:/studies/defectprediction/structure_metrics_files/libxml2.csv,C:/studies/defectprediction/structure_metrics_files/lighttpd.csv,C:/studies/defectprediction/structure_metrics_files/mpsolve.csv,C:/studies/defectprediction/structure_metrics_files/parrot.csv,C:/studies/defectprediction/structure_metrics_files/vim.csv
#These files are used to get commit list for commit-based prediction. We want to use them to calculate metrics
CommitCSVFilesCommitBasedPrediction=C:/studies/defectprediction/commits_blender.csv
#,C:/studies/defectprediction/commits_busybox.csv,C:/studies/defectprediction/commits_emacs.csv,C:/studies/defectprediction/commits_gimp.csv,C:/studies/defectprediction/commits_gnumeric.csv,C:/studies/defectprediction/commits_gnuplot.csv,C:/studies/defectprediction/commits_irssi.csv,C:/studies/defectprediction/commits_libxml2.csv,C:/studies/defectprediction/commits_lighttpd.csv,C:/studies/defectprediction/commits_mpsolve.csv,C:/studies/defectprediction/commits_parrot.csv,C:/studies/defectprediction/commits_vim.csv
#Regular expression that specifies tangled features
MultipleFeatureRegex = _AND_|_OR_
#Indicate whether metrics are aggregated over files or features: Enum values: Feature, File. In the study on defect prediction we use Feature aggregation
MetricCalculationBasis=Feature
#Indicate whether metrics are aggregated over commits (all commits to the current) or over releases
MetricCalculationCommitBased=true
#Print metrics per commit in addition to a summary file that aggregates values per file or feature
PrintMetricDetails=false
#Specify symbol that is used to fully qualify features
FeatureQualifiedNameSeparator=::
#When mapping multiple files to a feature or features, how are they separated in the maping file
MultiFileMappingSeperator=,
TextEncoding=UTF-8
#How are feature model files named
FeatureModelFile=.vp-project,FeatureModel.cfr
#How are folder mapping files named
FolderMappingFile=.vp-folder
#How are file mapping files named
FileMappingFile=.vp-files
#Regular expresssions for annotations
AllAnnotationPatterns=\\s*//&begin(.*)\\[(.*)\\]|(.*)//&line\\s*\\[(.*)\\]|\\s*((#\\s*ifndef\\s+.*)|(#\\s*ifdef\\s+.*)|(#\\s*if\\s+.*))
FragmentAnnotationBegin=\\s*//&begin(.*)\\[(.*)\\]
FragmentAnnotationEnd=\\s*//&end(.*)\\[(.*)\\]
LineAnnotation=(.*)//&line\\s*\\[(.*)\\]
AnnotatedLineIsImmediatelyAfterLineAnnotation=true
#When a block or line matches any of the above regexes, how do you extract the actual feature name from the line matched
EAGetFeatureRegex=((.*)//\\&line)|((.*)//\\&begin)|((.*)//\\&end)
#regexes for preprocessor-based annotations
IfDefBegin=\\s*((#\\s*ifndef\\s+.*)|(#\\s*ifdef\\s+.*)|(#\\s*elif\\s+.*)|(#\\s*if\\s+.*))
IfDefEnd=\\s*(#\\s*endif)|(#\\s*else)
IfDefGetFeatureRegex=(#\\s*ifndef)|(#\\s*ifdef)|(#\\s*elif)|(#\\s*if)
CamelCaseSplitRegex=(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])
#Regex compilers throw exceptions when they meat with math operators in certain integer features e.g., #if EXTRUDERS > 1
#So we replace all math operators with words
TextReplacementsFromFeatures=\\s*!=\\s*:_NOTEQUALTO_,\\s*>=\\s*:_GREATERorEQUALTO_,\\s*>\\s*:_ISGREATERTHAN_,\\s*<=\\s*:_LESSorEQUALTO_,\\s*<\\s*:_ISLESSTHAN_,\\s*==\\s*:_EQUALS_,\\(:EMPTY,\\):EMPTY,!\\s*:NOT_
#Indicate true if you want to focus only on boolean features
IgnoreIntegerOptions=true
#Specify which features to ignore provided they contain the following characters.
UnFitFeatureStartString=<html><head>,!=,>=,<=,>,<,==,!,0
SingleLineComment=//{2}|(/\\*)
#For simulation projects, specify author names for commits that must be considered
SimulationCommitAuthors=Wenbin Ji
#Only these file types will be anlaysed for annotations
AllowedFileExtensions=.js,.c,.cpp,.h,.cc,.y,.py,.java
#.js,.c,.cpp,.h,.java,.vp-folder,.vp-files,.py
#This command is used to get a log of all commits with the details
#date | full commit hash | author | commit message
SimulationCommitsGitLogCommand=git --no-pager log --graph --pretty=format:'_%H _%ad _%an _%s' --date=iso
#For each commit we reset the repository to the state at tha commit
CommitCheckoutCommand=git reset --hard
#What symbols are used for multi-feature annoations e.g. //&.begin [featureA & featrureB | featureC]
MultiFeatureAnnotationSeparator=AND|OR
#Which source code granularity level is to be processed for training data and test data
CodeAbstractionLevel=FRAGMENT
#The following 4  propeties refer to GraphEdit Distance metrics which we no longer support
CallGraphNonLabeledNodeName=anon
CallGraphNonLabeledLineName=null
ProjectLanguage=.js,.cpp
UseGraphEditDistanceMetric=false
#When saving features as labels in the multilabel dataset ARFF format, we can choose to use fully wualified names or just names
#E.g. for feature ClaferMooVisualizer::Client::Views, if this property is set to false, then the label in the dataset will only use 'Views'
UseFullFeatureNamesInMLDataFile=false
#This setting is no longer relevant
ExperimentDataFolder=C:/fanas/2/dataFiles
#Which library to use for learning algorthms. Enum values: MULAN, MEKA
#Currently only MULAN is supported
MLMethods=MULAN
#What evaluation method should be used. Enum: CV,TEST
#CV-kfold corss validation, you must specify the K. TEST-provide test dataset
#WE NO USE CRosS VALIDATION
EvaluationMethod=CV
KFolds=10
#For esembles such as Enseblmes of Binary relevance EBR, Classifier Chains (CC), Enselmbles of Classifier Chains (ECC), how many internal iterations must be performed
ChainIterations=3
#This property is no longer used
UnlabledAssetComparision=allneighbors
#This property is no longer used
UnlabledAssetComparisionKNN=10
#Which classifers should be used for Evaluation (CV)
#The full list includes:BR,CC,EBR,ECC,EPS,IBLR,LP,MLkNN,MLS,PS,RAkEL,RAkELd, AdaBoost.MH,CLR
EvaluationModels=BR,CC,EPS,IBLR,LP,MLkNN,MLS,PS,RAkELd
#Classfiers used for n+th predictions in the experiment
PredictionModels=BR,LP,RAkELd
ClassifierForCombinedResults=RAkELd
#This will be used when setting final models to be used for real development predictions
FinalPredictionModels=BR,CC,EBR,ECC,EPS,IBLR,LP,MLkNN,MLS,PS,RAkEL,RAkELd
#This setting enables printing of detailed BIbary partition and ranking results by each classfier for each instance predicted
PrintDetailedPredictionResults=true
#When ranking labels, classfiers often rank even labels that were not marked as relvant by the binary partition method, we use this setting to rank only relvant ones
#And use them as the predicted labels
RankRelevantLabelsOnly=true
#When trying to predict fragments, the changed code can be split into clustrs of a given threshold
#For instance, if 100 lines of code in a file have been changed, to create fragment level instances for prediction, we can split these 100 into clusters of 20
#Any number can be specified here deepnding on the project, e.g., 20, 15, 10, 5
FragmentClusterThreshold=20
#Changed lines of code can be split into clusters or be treated according to how they were changed as given by the diffs
#Enum values: THRESHOLD, DIFF
#For instance, if 100 lines of code have been changed, 50 of these might be consecutive lines in one diff and 30 in another diff and another 20 in another diff. hence
#FeatRacer would generate 3 clusters from the 100. However, when using the THRESHOLD method, the 100 are split into 5 equal parts given that the THRESHOLD is 20.
FragmentClusterMethod=DIFF
#This poperty is no longer relevant
IncludeScenarioCalculation=false
#NOTE: most of the below setting work when the ExecutionMethod is DE or E i.e. only experiment mode is included.
#Establishes the procedure to generate the labelsets for synthetic instances. The valid values for this parameter are the following:
#
#    1 -> The synthetic labelset will be the intersection of the neighbors' labelsets.
#    2 -> The synthetic labelset will be the union of the neighbors' labelsets.
#    3 -> The synthetic labelset will be generated using a ranking of labels in the neighbors' labelsets.
MLSMOTELabelCombination=3
#Indicates whether the training should be done on balanced datasets (assuming they already exist)
UseDataBalancer=false
#Indcates whether FeatRacer should balance datasets
RunDatabalancer=false
# Consider a file md_control.js that belongs to the feature ClaferMooVisualizer::Client::Views::Control
#Hierachical labeling would indicate that file md_control.js is mapped to feature 'control' and all its parents: 'views' and 'client'
#This has potential to cause imbalnce since feature 'client' is mapped to a folder and this means it will be a very dominant feature in the dataset
#Thus, to avoid this, we can diable hierachical labeling and only label assets with their actual mapped features; 'control' in this case
UseHierachicalLabeling=false
#When generating datasets in WEKA's ARFF format, we save them as DenseInstance or SparseInstance.
#It is unsual for multilabel datasets to be dense since for many instances, many labels may have '0' values. hence to improve performance,
# we save them as sparse instances in which only non zero values are saved by idicating their indexes
UseSparseInstance=true
#Use this option to generate statistics about datasets e.g., number of instances, label cardinatlity, imbalance ratio, scumble, etc
GenerateDatasetStatistics=false
#Indicates whether the experiment should be run
RunExperiment=true
RunCrossValidation=false
RunPredictions=true
#This property is no longer relevant
OrderCommitsByCloneOrder=false
#This property is used to combine different CSV files, e.g., for datasets, for prediction results, etc
#When excuting featracer, the different paths taken are:
#RunData OR RunProjectStatistics (see below) OR Generate data (D) OR Generate Data and Run Experiment (DE) OR Run Experiment only (E)
RunRDataPlots=false
#When the experiment is done, copy results to the RDataFolder specified below to ease analysis
CopyExpResultsToRFolder=true
#Folder where you want to analyse results from
RDataFolder=C:/exp/expnew2
#Some metrics could have large numbers, e.g., nunber of commits (COMM) and nuimber of Annotations (NEA) which are dispropotianate to other values e.g., cosine simlarity (CSM)
#Hence we can normalise these to unify comparison and prevent bias by classfiers
NormaliseData=false
#From the list of metrics, we can specify which ones to normalise
#Full list is CSM,SLD,NEA,COMMITS
#Howver, CSM and SLD are already within range 0 to 1
AttributesToNormalise=DDEV,COMM,CCC,ACCC,NLOC,DNFMA,NFMA
#Normalisation strategy can be Weka's filter methods or FestRacer's own implemtaion of normalisation
#FTR uses the max min normalisation: (value - min) / (max - min); values are FTA or WEKA
NormalisationStrategy=WEKA
#When excuting experiments, we can start with first ranking metrics
PerformFeatureSelection=false
#Some projects have large and many datasets. Instead of running through all datasets to calcluate ranks for features, we can sample
#E.g., if a project has 1200 datasets (from 1200 commits), we can choose to perfom feature selection for every 100th dataset, thus reducing the number to 12
PerformFeatureSelectionForEveryKDataset=30
#Here we use Information Gain and ReliefF methods
#The full list is
#IG-BR,IG-LP,RF-BR,RF-LP
#However, RF has been shown to be better performing than IG
FeatureSelectionMethods=RF-BR,RF-LP
#Here you can choose what final value you get for the rank of a feature
FeatureSelectionRank=overall_rank
TopNFeaturesMethod=K
TopNFeatures=8
PersistProjectData=false
ProjectCopiedRepositoriesForStats=C:/fanas/2/ClaferConfigurator,C:/fanas/2/ClaferIDE,C:/fanas/2/ClaferMooVisualizer,C:/fanas/2/ClaferToolsUICommonPlatform,C:/fanas/2/Marlin
PrintProjectStats=false
UseOnlyAssetsWithKnownLabelsForExperiment=true
UseMaxForPredictionMetricAggregation=false
GitBlameCommand=git blame
#Used when combining training and test data from muitple projects [not related to FeatRacer]
ProjectCombinations=10
#number of projects to include in training daatset
TrainingSetSize=9
#number of projects to inlude in
#Get annotations from cureent state of project wihtout uring commits
GetAnnotationsWithoutCommits=false
RunWithRepoDriller=true
NumberOfThreads=40
CommitsForFeatRacerExperiment=C:/studies/commits/viz.txt,C:/studies/commits/ide.txt,C:/studies/commits/config.txt,C:/studies/commits/tools.txt
PerformFeatureCounts=true
#####THIS SECTION IS FOR DEFECT PREDICTION####
TrainDataFile=C:/studies/defectprediction/procmet/features_75p_train.arff,C:/studies/defectprediction/procmet/files_only_75_train.arff
TestDataFile=C:/studies/defectprediction/procmet/features_75p_test_unlabeled.arff,C:/studies/defectprediction/procmet/files_only_75_test_unlabeled.arff
FeatureFileMapping=C:/studies/defectprediction/procmet/feature_file_mapping.csv
LabeledFeaturesPath=C:/studies/defectprediction/procmet/features_75p_test_labeled.csv
LabeledFilesPath=C:/studies/defectprediction/procmet/files_only_75_test_labeled.csv
#M-map only, C-classify only, CM-classify and map features and files, FE- evaluation of classifiers for file-based predictions, AE-Attribute Evaluation
#CC-Change based predictions, XP-cross project predictions,LIBSVM-LIBSVM generator
DefectRunMode=XP
#here specify teh database to which featracer shall connect to store data. Values are seprated by comas as follows: databaseNameURL,username,password
DataBaseConnectionString=jdbc:mysql://localhost:3306/featracerdb,root,FeatRacer@!2021
SaveDataInDataBase=false
#Here specify parent folder for where to find commit-level and release-level datasets
CommitDataSetParentFolder=C:/studies/defectprediction/cc/commit
ReleaseDataSetParentFolder=C:/studies/defectprediction/cc/release
CommitFilesParentFolder=C:/studies/defectprediction
CrossProjectCommitFolder=C:/studies/defectprediction/xp/commit
CrossProjectReleaseFolder=C:/studies/defectprediction/xp/release
ProjectCombinationsFile=C:/studies/defectprediction/projectCombiinations.csv